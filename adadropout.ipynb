{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive dropout MNIST example\n",
    "\n",
    "I've pulled this example directly from the [PyTorch examples](https://github.com/pytorch/examples) github.\n",
    "\n",
    "### Notes\n",
    "\n",
    "I return the average loss of the training across the whole batch.\n",
    "A better approach might be to return the average of the last several batches to give a more accurate loss and avoid random fluctuations coming from just the last batch.\n",
    "I think we can get away with the whole batch average since the first few epochs where the larger training loss changes happen we're usually not too worried about overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '../data', train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "This is where the magic happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout=0.):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "#         self.dropout1 = nn.Dropout2d(0.25)\n",
    "#         self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # In the official example this is half the last dropout rate\n",
    "        x = F.dropout2d(x, self.dropout/2., training=self.training)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout2d(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction train/validation loops\n",
    "\n",
    "Nothing special done here except returning the last loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))\n",
    "        avg_loss += loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "    avg_loss /= len(train_loader.dataset)\n",
    "            \n",
    "    return avg_loss.item(), correct / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.285033\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.002698\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000800\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.749884\n",
      "\n",
      "Test set: Average loss: 0.0505, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train loss/acc: 0.00607/0.97, Test loss/acc: 0.05053/0.98, New dropout: 0.00000\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.000068\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.000098\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.000337\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.000456\n",
      "\n",
      "Test set: Average loss: 0.0381, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00182/0.99, Test loss/acc: 0.03809/0.99, New dropout: 0.00260\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.006139\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.000016\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.000001\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 9912/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00083/1.00, Test loss/acc: 0.03456/0.99, New dropout: 0.00507\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000020\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.000026\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001490\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.007722\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00044/1.00, Test loss/acc: 0.03631/0.99, New dropout: 0.00563\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.004805\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000105\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00025/1.00, Test loss/acc: 0.03707/0.99, New dropout: 0.00739\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001381\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.000247\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.000307\n",
      "\n",
      "Test set: Average loss: 0.0396, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00016/1.00, Test loss/acc: 0.03961/0.99, New dropout: 0.00727\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000056\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.000036\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00013/1.00, Test loss/acc: 0.04010/0.99, New dropout: 0.00755\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0419, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00012/1.00, Test loss/acc: 0.04194/0.99, New dropout: 0.00760\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0424, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00010/1.00, Test loss/acc: 0.04243/0.99, New dropout: 0.00799\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000052\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0431, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00009/1.00, Test loss/acc: 0.04306/0.99, New dropout: 0.00812\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.000053\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.000007\n",
      "\n",
      "Test set: Average loss: 0.0433, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00008/1.00, Test loss/acc: 0.04334/0.99, New dropout: 0.00819\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.000091\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0433, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00008/1.00, Test loss/acc: 0.04334/0.99, New dropout: 0.00805\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.000155\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.000038\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.000063\n",
      "\n",
      "Test set: Average loss: 0.0437, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00008/1.00, Test loss/acc: 0.04368/0.99, New dropout: 0.00794\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.000002\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9917/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00008/1.00, Test loss/acc: 0.04383/0.99, New dropout: 0.00807\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.000029\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.000037\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0439, Accuracy: 9918/10000 (99%)\n",
      "\n",
      "Train loss/acc: 0.00008/1.00, Test loss/acc: 0.04389/0.99, New dropout: 0.00800\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-113:\n",
      "Traceback (most recent call last):\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 294, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/multiprocessing/connection.py\", line 619, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-da0f7108a9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f19eb860efe0>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/portal/ekpbms1/home/jkahn/miniconda3/envs/ml/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc = train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "#     dropout = 1. - (train_loss / test_loss)\n",
    "    dropout = 1. - (test_acc / train_acc)\n",
    "    dropout = max(dropout, 0.)  # Ensure it doesn't dip below zero\n",
    "    dropout = min(dropout, 0.5)  # Set upper limit on loss\n",
    "    print(f'Train loss/acc: {train_loss:.5f}/{train_acc:.3f}, Test loss/acc: {test_loss:.5f}/{test_acc:.3f}, New dropout: {dropout:.5f}')\n",
    "    model.set_dropout(dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
